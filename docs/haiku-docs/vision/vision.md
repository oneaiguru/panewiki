---
id: vision-overview
title: "Why This Matters: The Paradigm Shift"
models: [opus]
summary: true
readTime: 3m
---

<!-- model: opus -->
> **Path:** Home › Vision
> **Validation:** Reviewed by Sonnet ✓

# Why This Matters: The Paradigm Shift

## The Fundamental Problem

**Current AI interfaces violate human cognition:**

```
AI Output:    10,000 tokens (complete)
Human Brain:  100 tokens (processing window)
Result:       9,900 tokens never read
```

Users don't read walls of text. They skim first, decide relevance, then drill.

But AI tools deliver complete content upfront. Mismatch = information overload.

## Three Critical Insights

### 1. Information Architecture Matters More Than Content

```
OLD: "Here's everything" (10,000 tokens)
     User: Lost. Overwhelmed. Leaves.

NEW: "Here's essence" (100 tokens)
     User: Understands direction
     → Clicks for details as needed
     → Engages deeper → reads MORE total
```

**Paradox:** Showing LESS initially makes users read MORE overall.

### 2. Reading Width Affects Comprehension

Decades of newspaper + web research prove:
- 80+ char lines: eye strain, comprehension -15%
- 60-75 char lines: optimal, comprehension +20%
- Too narrow: awkward wrapping, engagement down

**400px column = 60-75 characters = optimal reading.**

It's not aesthetic. It's cognitive science.

### 3. Not All AI Work Costs The Same

```
Opus:  Strategic thinking (premium, see Pricing)
Haiku: Execution (low-cost, see Pricing)
       Huge delta → orchestrate intentionally
```

Rates: see [Pricing](../appendix/pricing) for current input/output costs.

**Smart orchestration:** Opus thinks (once, expensive), Haiku executes (parallel, cheap), Sonnet validates (moderate, reliable).

Result: ≈81% cost reduction (scenario in [Pricing](../appendix/pricing)) + BETTER quality.

## The Paradigm Shift

```
FROM: Linear scrolling          TO: Layered navigation
  ↓ One model                       ↓ Three models
  ↓ One pass                        ↓ Multi-pass verification
  ↓ Complete output                 ↓ Summary → Detail on demand
  ↓ Users overwhelmed               ↓ Users engaged + informed
```

## Why Now?

1. **LLM pricing enables it** - Cheap models for execution (2024)
2. **Token budgets matter** - Every token = real cost
3. **Cognitive load is real** - Brain hasn't changed
4. **Parallel execution works** - Generate multiple components simultaneously
5. **Transparency teaches** - Users learn optimal AI patterns

## What Changes

✓ Users see essence first (30 lines) not everything (10,000 lines)
✓ Architects think in layers (summary, detail) not linear flows
✓ Developers use 3 models (each for their strength) not 1 for all
✓ Teams save 87% on API costs while improving quality
✓ Learning curve becomes interface teaching tool

---

---
**Related**
- [Next: Problem Statement](problem-statement)
- [See also: Solution Overview](solution-overview)
- [Back: Home](../home)
